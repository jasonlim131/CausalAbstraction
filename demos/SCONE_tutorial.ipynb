{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71285309",
   "metadata": {},
   "source": [
    "# SCONE Causal Abstraction Experiment\n",
    "\n",
    "This notebook implements a causal abstraction experiment on the SCONE (Semantic Composition through Negation) dataset. The experiment investigates how large language models process logical entailment by analyzing causal relationships between semantic variables and neural representations.\n",
    "\n",
    "## Overview\n",
    "- **Dataset**: SCONE entailment dataset with negation scoping\n",
    "- **Model**: Llama-3.1-8B-Instruct\n",
    "- **Method**: Causal intervention analysis using PyVene\n",
    "- **Target**: Understanding how models process logical scope and entailment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58e33fd",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31a8aac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy<2 in /data/jason_lim/CausalAbstraction/venv/lib/python3.12/site-packages (1.26.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade \"numpy<2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f1f540a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnsight is not detected. Please install via 'pip install nnsight' for nnsight backend.\n",
      "Built with CUDA: 12.1\n",
      "cuda.is_available(): False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import torch\n",
    "import re\n",
    "import pyvene as pv\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def setup_cuda():\n",
    "    \"\"\"Check CUDA availability and setup.\"\"\"\n",
    "    print(\"Built with CUDA:\", torch.version.cuda)\n",
    "    available = torch.cuda.is_available()\n",
    "    print(\"cuda.is_available():\", available)\n",
    "    if available:\n",
    "        print(\"Device name:\", torch.cuda.get_device_name(0))\n",
    "    return available\n",
    "\n",
    "def add_project_root(root_path=\"/data/jason_lim/CausalAbstraction\"):\n",
    "    \"\"\"Add project root to Python path.\"\"\"\n",
    "    if root_path not in sys.path:\n",
    "        sys.path.append(root_path)\n",
    "    return root_path\n",
    "\n",
    "# Setup\n",
    "add_project_root()\n",
    "cuda_ok = setup_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2419403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internal module imports\n",
    "from causal.causal_model import CausalModel, CounterfactualDataset\n",
    "from experiments.scone_filter_experiment import SconeFilterExperiment\n",
    "from lm_units.fixed_pipeline import LMPipeline\n",
    "from lm_units.LM_units import TokenPosition, get_last_token_index\n",
    "from experiments.residual_stream_experiment import PatchResidualStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b7eaa37-65a5-4019-a849-72dd831d68d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /data/jason_lim/CausalAbstraction\n",
      "Path added to sys.path: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Set the working directory\n",
    "os.chdir('/data/jason_lim/CausalAbstraction')\n",
    "\n",
    "# Add to Python path\n",
    "sys.path.append('/data/jason_lim/CausalAbstraction')\n",
    "\n",
    "# Verify the changes\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Path added to sys.path: {'/data/jason_lim/CausalAbstraction' in sys.path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6f6c5e",
   "metadata": {},
   "source": [
    "## 2. Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9ee37aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 datasets:\n",
      "  no_negation: 1202 examples\n",
      "  one_scoped: 1202 examples\n",
      "  one_not_scoped: 1202 examples\n",
      "Loaded 499 entailment entries\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def load_scone_datasets(data_dir=\"data\", half=True):\n",
    "    \"\"\"Load and combine SCONE datasets.\"\"\"\n",
    "    if half:\n",
    "        dataset_names = [\"no_negation\", \"one_scoped\", \"one_not_scoped\"]\n",
    "    else:\n",
    "        dataset_names = [\"no_negation\", \"one_scoped\", \"one_not_scoped\",\n",
    "                        \"one_scoped_one_not_scoped\", \"two_scoped\", \"two_not_scoped\"]\n",
    "    \n",
    "    combined_datasets = {}\n",
    "    for name in dataset_names:\n",
    "        train_df = pd.read_csv(f\"{data_dir}/train/{name}.csv\")\n",
    "        test_df = pd.read_csv(f\"{data_dir}/test/{name}.csv\")\n",
    "        combined_datasets[name] = pd.concat([train_df, test_df], ignore_index=True)\n",
    "    \n",
    "    return combined_datasets\n",
    "\n",
    "def load_entailment_data(file_path=\"data/lookup/entailment_lookup_filtered.jsonl\"):\n",
    "    \"\"\"Load entailment lookup data.\"\"\"\n",
    "    entailment_data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                entry = json.loads(line)\n",
    "                entailment_data.append(entry)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Failed to parse line: {line}\")\n",
    "    return entailment_data\n",
    "\n",
    "# Load data\n",
    "datasets_dict = load_scone_datasets(half=True)\n",
    "entailment_data = load_entailment_data()\n",
    "\n",
    "print(f\"Loaded {len(datasets_dict)} datasets:\")\n",
    "for name, df in datasets_dict.items():\n",
    "    print(f\"  {name}: {len(df)} examples\")\n",
    "print(f\"Loaded {len(entailment_data)} entailment entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e67bf74",
   "metadata": {},
   "source": [
    "## 3. Input/Output Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b839b967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_dumper(example):\n",
    "    \"\"\"Convert example to prompt format.\"\"\"\n",
    "    if isinstance(example, str):\n",
    "        return example\n",
    "    \n",
    "    premise = example.get(\"PremiseSentence\", \"\")\n",
    "    hypothesis = example.get(\"HypothesisSentence\", \"\")\n",
    "    \n",
    "    prompt = (\n",
    "        f\"Sentence 1: {premise}\\n\"\n",
    "        f\"Sentence 2: {hypothesis}\\n\"\n",
    "        f\"Does Sentence 1 entail Sentence 2? Please respond only with either 'entailment' or 'neutral'.\\n\"\n",
    "        \"Answer: \"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def output_dumper(setting):\n",
    "    \"\"\"Extract final relation from setting.\"\"\"\n",
    "    return setting['FinalRelation']\n",
    "\n",
    "def checker(neural_output, causal_output, debug=False):\n",
    "    \"\"\"Enhanced checker that handles both old and new formats.\"\"\"\n",
    "    # Handle the case where causal_output is a string (old format)\n",
    "    if isinstance(causal_output, str):\n",
    "        expected_label = causal_output\n",
    "    # Handle case where it's a dict with raw_output\n",
    "    elif isinstance(causal_output, dict) and \"raw_output\" in causal_output:\n",
    "        expected_label = causal_output[\"raw_output\"]\n",
    "    else:\n",
    "        expected_label = str(causal_output)\n",
    "    \n",
    "    # Simple containment check\n",
    "    return expected_label.lower().strip() in str(neural_output).lower().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5109b8",
   "metadata": {},
   "source": [
    "## 4. Model Pipeline Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0779a8eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16cf25b0613742408be8008f69a04326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model on cpu\n"
     ]
    }
   ],
   "source": [
    "def init_pipeline(model_name=\"meta-llama/Llama-3.1-8B-Instruct\"):\n",
    "    \"\"\"Initialize the language model pipeline.\"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    pipe = LMPipeline(model_name, max_new_tokens=10, device=device, dtype=torch.float16)\n",
    "    pipe.tokenizer.padding_side = \"right\"\n",
    "    print(f\"Loaded model on {pipe.model.device}\")\n",
    "    return pipe\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = init_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cbcf24",
   "metadata": {},
   "source": [
    "## 5. Token Position Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617db0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicate_premise(prompt, pipeline, debug=False):\n",
    "    \"\"\"Return token indices for the premise predicate.\"\"\"\n",
    "    try:\n",
    "        tokens = pipeline.tokenizer.tokenize(prompt)\n",
    "        \n",
    "        # Look for \"Sentence 1:\" and find predicate after it\n",
    "        for i in range(len(tokens) - 1):\n",
    "            if 'sentence' in tokens[i].lower() and '1' in tokens[i+1]:\n",
    "                search_start = i + 2\n",
    "                search_end = min(search_start + 20, len(tokens))\n",
    "                \n",
    "                for j in range(search_start, search_end):\n",
    "                    clean_token = tokens[j].lower().replace('▁', '')\n",
    "                    if len(clean_token) > 3 and clean_token not in [\"that\", \"with\", \"from\", \"this\"]:\n",
    "                        return [j]\n",
    "                \n",
    "                return [search_start + 3]  \n",
    "        \n",
    "        return [5]  \n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"Error in get_predicate_premise: {e}\")\n",
    "        return [5]\n",
    "\n",
    "def get_predicate_hypothesis(prompt, pipeline, debug=False):\n",
    "    \"\"\"Return token indices for the hypothesis predicate.\"\"\"\n",
    "    try:\n",
    "        tokens = pipeline.tokenizer.tokenize(prompt)\n",
    "        \n",
    "        # Look for \"Sentence 2:\" and find predicate after it\n",
    "        for i in range(len(tokens) - 1):\n",
    "            if 'sentence' in tokens[i].lower() and '2' in tokens[i+1]:\n",
    "                search_start = i + 2\n",
    "                search_end = min(search_start + 20, len(tokens))\n",
    "                \n",
    "                for j in range(search_start, search_end):\n",
    "                    clean_token = tokens[j].lower().replace('▁', '')\n",
    "                    if len(clean_token) > 3 and clean_token not in [\"that\", \"with\", \"from\", \"this\"]:\n",
    "                        return [j]\n",
    "                \n",
    "                return [search_start + 3]\n",
    "        \n",
    "        return [15]  # Default fallback\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"Error in get_predicate_hypothesis: {e}\")\n",
    "        return [15]\n",
    "\n",
    "def get_negation_index(prompt, pipeline, debug=False):\n",
    "    \"\"\"Return token indices for negation terms.\"\"\"\n",
    "    try:\n",
    "        tokens = pipeline.tokenizer.tokenize(prompt)\n",
    "        \n",
    "        # Look for negation terms\n",
    "        negation_terms = [\"not\", \"n't\", \"no\", \"never\", \"none\"]\n",
    "        for i, token in enumerate(tokens):\n",
    "            clean_token = token.lower().replace('▁', '')\n",
    "            for term in negation_terms:\n",
    "                if term in clean_token:\n",
    "                    return [i]\n",
    "        \n",
    "        return [3]  # Fallback if no negation found\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"Error in get_negation_index: {e}\")\n",
    "        return [3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879afd35",
   "metadata": {},
   "source": [
    "## 6. Causal Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0250d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mechanism functions\n",
    "def create_raw_input(premise_sentence, hypothesis_sentence):\n",
    "    \"\"\"Create the raw input prompt from premise and hypothesis.\"\"\"\n",
    "    if premise_sentence is None or hypothesis_sentence is None:\n",
    "        return None\n",
    "    \n",
    "    prompt = (\n",
    "        f\"Sentence 1: {premise_sentence}\\n\"\n",
    "        f\"Sentence 2: {hypothesis_sentence}\\n\"\n",
    "        f\"Does Sentence 1 entail Sentence 2? Answer only 'entailment' or 'neutral'. \\n \"\n",
    "        \"Answer: \"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def create_raw_output(final_relation):\n",
    "    \"\"\"Create the raw output from final relation.\"\"\"\n",
    "    return final_relation\n",
    "\n",
    "def compute_base_entailment(pred1, pred2):\n",
    "    \"\"\"Compute base entailment between predicates using entailment lookup.\"\"\"\n",
    "    entailment_lookup = {}\n",
    "    for entry in entailment_data:\n",
    "        if isinstance(entry, dict) and entry.get('label') == 1:\n",
    "            entailment_lookup[(entry['hyponym'], entry['hypernym'])] = \"entailment\"\n",
    "    return entailment_lookup.get((pred1, pred2), \"neutral\")\n",
    "\n",
    "def compute_final_relation(base_entailment, scope):\n",
    "    \"\"\"Pure function: compute final relation from base entailment and scope\"\"\"\n",
    "    if scope in [\"no_negation\", \"one_not_scoped\"]:  # Even scopes\n",
    "        return base_entailment  # Pass through\n",
    "    elif scope == \"one_scoped\":  # Odd scope  \n",
    "        return \"neutral\" if base_entailment == \"entailment\" else \"entailment\"  # Flip\n",
    "    else:\n",
    "        return base_entailment  # Default fallback\n",
    "\n",
    "def get_premise_sentence(pred1, pred2, scope, datasets_dict):\n",
    "    \"\"\"Get premise sentence based on predicates and scope.\"\"\"\n",
    "    if scope not in datasets_dict:\n",
    "        return None\n",
    "        \n",
    "    df = datasets_dict[scope]\n",
    "    \n",
    "    if scope == \"one_scoped\":\n",
    "        sent1_col = 'sentence1'\n",
    "    else:\n",
    "        sent1_col = 'sentence1_edited' if 'sentence1_edited' in df.columns else 'sentence1'\n",
    "    \n",
    "    matches = df[(df['sentence1_lex'] == pred1) & (df['sentence2_lex'] == pred2)]\n",
    "    \n",
    "    if matches.empty:\n",
    "        return None\n",
    "        \n",
    "    chosen_row = matches.sample(n=1).iloc[0]\n",
    "    return chosen_row[sent1_col]\n",
    "\n",
    "def get_hypothesis_sentence(pred1, pred2, scope, datasets_dict):\n",
    "    \"\"\"Get hypothesis sentence based on predicates and scope.\"\"\"\n",
    "    if scope not in datasets_dict:\n",
    "        return None\n",
    "        \n",
    "    df = datasets_dict[scope]\n",
    "    \n",
    "    if scope == \"one_scoped\":\n",
    "        sent2_col = 'sentence2'\n",
    "    else:\n",
    "        sent2_col = 'sentence2_edited' if 'sentence2_edited' in df.columns else 'sentence2'\n",
    "    \n",
    "    matches = df[(df['sentence1_lex'] == pred1) & (df['sentence2_lex'] == pred2)]\n",
    "    \n",
    "    if matches.empty:\n",
    "        return None\n",
    "        \n",
    "    chosen_row = matches.sample(n=1).iloc[0]\n",
    "    return chosen_row[sent2_col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec854019",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SconeCausalModel(CausalModel):\n",
    "    \"\"\"SCONE-specific causal model for entailment reasoning.\n",
    "        Inherits from CausalModel and implements specific mechanisms for SCONE datasets.\"\"\"\n",
    "    \n",
    "    def __init__(self, datasets_dict, entailment_data, variables, values, parents, mechanisms):\n",
    "        super().__init__(variables, values, parents, mechanisms, id=\"SCONE_entailment\")\n",
    "        self.datasets_dict = datasets_dict\n",
    "        self.entailment_data = entailment_data\n",
    "        self.current_sentence_pair = None\n",
    "        self.current_gold_label = None\n",
    "        \n",
    "        # Create entailment lookup from entailment_data\n",
    "        self.entailment_lookup = {}\n",
    "        for entry in entailment_data:\n",
    "            if isinstance(entry, dict) and entry.get('label') == 1:\n",
    "                self.entailment_lookup[(entry['hyponym'], entry['hypernym'])] = \"entailment\"\n",
    "    \n",
    "    def run_forward(self, intervention=None, debug=False):\n",
    "        \"\"\"Reset cached data at start of forward pass.\"\"\"\n",
    "        self.current_sentence_pair = None\n",
    "        self.current_gold_label = None\n",
    "        result = super().run_forward(intervention=intervention)\n",
    "        return result\n",
    "    \n",
    "    def label_counterfactual_data(self, dataset, target_variables):\n",
    "        \"\"\"Proper intervention: Take original input, change only target variables, then recompute through causal model.\"\"\"\n",
    "        inputs = []\n",
    "        counterfactual_inputs_list = []\n",
    "        labels = []\n",
    "        settings = []\n",
    "        \n",
    "        for i, example in enumerate(dataset):\n",
    "            input_data = example[\"input\"] \n",
    "            counterfactual_inputs = example[\"counterfactual_inputs\"]\n",
    "            \n",
    "            if isinstance(counterfactual_inputs, list):\n",
    "                counterfactual_input = counterfactual_inputs[0]\n",
    "            else:\n",
    "                counterfactual_input = counterfactual_inputs\n",
    "            \n",
    "            # PROPER INTERVENTION: Start with original, change only target variables\n",
    "            intervention_input = {}\n",
    "\n",
    "            # Always keep the exogenous variables (predicates)\n",
    "            intervention_input[\"Predicate1\"] = input_data[\"Predicate1\"] \n",
    "            intervention_input[\"Predicate2\"] = input_data[\"Predicate2\"]\n",
    "            intervention_input[\"Scope\"] = input_data[\"Scope\"]\n",
    "\n",
    "            # Add the target variable intervention\n",
    "            for var in target_variables:\n",
    "                if var in counterfactual_input:\n",
    "                    intervention_input[var] = counterfactual_input[var]\n",
    "                                \n",
    "            # Recompute through causal model with intervention\n",
    "            intervention_result = self.run_forward(intervention=intervention_input)\n",
    "            expected_output = intervention_result[\"raw_output\"]\n",
    "            \n",
    "            # Create setting with intervention values\n",
    "            setting = {}\n",
    "            for var in target_variables:\n",
    "                if var in intervention_input:\n",
    "                    setting[var] = intervention_input[var]\n",
    "            setting[\"raw_output\"] = expected_output\n",
    "            \n",
    "            inputs.append(input_data)\n",
    "            counterfactual_inputs_list.append(example[\"counterfactual_inputs\"])\n",
    "            labels.append(expected_output)\n",
    "            settings.append(setting)\n",
    "        \n",
    "        data_dict = {\n",
    "            \"input\": inputs,\n",
    "            \"counterfactual_inputs\": counterfactual_inputs_list,\n",
    "            \"label\": labels,\n",
    "            \"setting\": settings\n",
    "        }\n",
    "        \n",
    "        dataset_id = getattr(dataset, 'id', 'unknown')\n",
    "        target_vars_str = \"_\".join(target_variables)\n",
    "        return CounterfactualDataset.from_dict(data_dict, id=f\"{dataset_id}_{target_vars_str}_labeled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8a889f",
   "metadata": {},
   "source": [
    "## 7. Counterfactual Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354260f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_entailment_counterfactual(datasets_dict, scone_model, debug=False):\n",
    "    \"\"\"Create base entailment counterfactual by finding pairs with opposite BaseEntailment values.\"\"\"\n",
    "    allowed_scopes = [s for s in ['no_negation', 'one_scoped', 'one_not_scoped'] \n",
    "                     if s in datasets_dict]\n",
    "    if not allowed_scopes:\n",
    "        raise ValueError(\"None of the allowed scopes are available in datasets_dict.\")\n",
    "    \n",
    "    # Sample original input\n",
    "    orig_scope = random.choice(allowed_scopes)\n",
    "    df_orig = datasets_dict[orig_scope]\n",
    "    \n",
    "    # Sample a random row\n",
    "    idx = random.randint(0, len(df_orig) - 1)\n",
    "    row_orig = df_orig.iloc[idx]\n",
    "    \n",
    "    # Extract predicate pair\n",
    "    pred1 = row_orig.get(\"sentence1_lex\", None)\n",
    "    pred2 = row_orig.get(\"sentence2_lex\", None)\n",
    "    \n",
    "    # Build original input\n",
    "    original_input = {\n",
    "        \"Scope\": orig_scope,\n",
    "        \"Predicate1\": pred1,\n",
    "        \"Predicate2\": pred2\n",
    "    }\n",
    "    \n",
    "    # Run through causal model to get BaseEntailment\n",
    "    original_result = scone_model.run_forward(intervention=original_input)\n",
    "    original_base_entailment = original_result[\"BaseEntailment\"]\n",
    "    \n",
    "    # Determine target BaseEntailment (opposite of original)\n",
    "    target_base_entailment = \"neutral\" if original_base_entailment == \"entailment\" else \"entailment\"\n",
    "    \n",
    "    # Search for a counterfactual with opposite BaseEntailment\n",
    "    max_attempts = 50\n",
    "    counterfactual_found = False\n",
    "    \n",
    "    for _ in range(max_attempts):\n",
    "        # Sample random input from any allowed scope\n",
    "        random_scope = random.choice(allowed_scopes)\n",
    "        df_random = datasets_dict[random_scope]\n",
    "        random_idx = random.randint(0, len(df_random) - 1)\n",
    "        row_random = df_random.iloc[random_idx]\n",
    "        \n",
    "        # Get predicates\n",
    "        random_pred1 = row_random.get(\"sentence1_lex\", None)\n",
    "        random_pred2 = row_random.get(\"sentence2_lex\", None)\n",
    "        \n",
    "        # Build candidate counterfactual\n",
    "        candidate_input = {\n",
    "            \"Scope\": random_scope,\n",
    "            \"Predicate1\": random_pred1,\n",
    "            \"Predicate2\": random_pred2\n",
    "        }\n",
    "        \n",
    "        # Check BaseEntailment using causal model\n",
    "        candidate_result = scone_model.run_forward(intervention=candidate_input)\n",
    "        candidate_base_entailment = candidate_result[\"BaseEntailment\"]\n",
    "        \n",
    "        if candidate_base_entailment == target_base_entailment:\n",
    "            counterfactual_input = candidate_result\n",
    "            counterfactual_found = True\n",
    "            break\n",
    "    \n",
    "    if not counterfactual_found:\n",
    "        retry = 0\n",
    "        while retry < 10:\n",
    "            cf_scope = random.choice(allowed_scopes)\n",
    "            df_cf = datasets_dict[cf_scope]\n",
    "            cf_idx = random.randint(0, len(df_cf) - 1)\n",
    "            \n",
    "            if cf_scope == orig_scope and cf_idx == idx:\n",
    "                retry += 1\n",
    "                continue\n",
    "                \n",
    "            row_cf = df_cf.iloc[cf_idx]\n",
    "            cf_pred1 = row_cf.get(\"sentence1_lex\", None)\n",
    "            cf_pred2 = row_cf.get(\"sentence2_lex\", None)\n",
    "            \n",
    "            fallback_input = {\n",
    "                \"Scope\": cf_scope,\n",
    "                \"Predicate1\": cf_pred1,\n",
    "                \"Predicate2\": cf_pred2\n",
    "            }\n",
    "            counterfactual_input = scone_model.run_forward(intervention=fallback_input)\n",
    "            break\n",
    "    \n",
    "    return {\"input\": original_result, \"counterfactual_inputs\": [counterfactual_input]}\n",
    "\n",
    "def create_scope_counterfactual(datasets_dict, scone_model, debug=False):\n",
    "    \"\"\"Create scope counterfactual by toggling between even/odd scopes with randomized predicates.\"\"\"\n",
    "    allowed_scopes = [s for s in ['no_negation', 'one_scoped', 'one_not_scoped'] \n",
    "                     if s in datasets_dict]\n",
    "    if not allowed_scopes:\n",
    "        raise ValueError(\"None of the allowed scopes are available in datasets_dict.\")\n",
    "    \n",
    "    # Choose an original scope at random\n",
    "    orig_scope = random.choice(allowed_scopes)\n",
    "    df_orig = datasets_dict[orig_scope]\n",
    "    orig_idx = random.randint(0, len(df_orig) - 1)\n",
    "    row_orig = df_orig.iloc[orig_idx]\n",
    "    orig_pred1 = row_orig.get(\"sentence1_lex\", None)\n",
    "    orig_pred2 = row_orig.get(\"sentence2_lex\", None)\n",
    "\n",
    "    even_scopes = [s for s in ['no_negation', 'one_not_scoped'] if s in allowed_scopes]\n",
    "    odd_scopes = [s for s in ['one_scoped'] if s in allowed_scopes]\n",
    "\n",
    "    if orig_scope in even_scopes and odd_scopes:\n",
    "        new_scope = odd_scopes[0]\n",
    "    elif orig_scope in odd_scopes and even_scopes:\n",
    "        new_scope = random.choice(even_scopes)\n",
    "    else:\n",
    "        available_scopes = [s for s in allowed_scopes if s != orig_scope]\n",
    "        new_scope = random.choice(available_scopes) if available_scopes else orig_scope\n",
    "\n",
    "    df_new = datasets_dict[new_scope]\n",
    "    new_idx = random.randint(0, len(df_new) - 1)\n",
    "    if new_scope == orig_scope:\n",
    "        attempts = 0\n",
    "        while new_idx == orig_idx and len(df_new) > 1 and attempts < 10:\n",
    "            new_idx = random.randint(0, len(df_new) - 1)\n",
    "            attempts += 1\n",
    "\n",
    "    row_new = df_new.iloc[new_idx]\n",
    "    new_pred1 = row_new.get(\"sentence1_lex\", None)\n",
    "    new_pred2 = row_new.get(\"sentence2_lex\", None)\n",
    "\n",
    "    original_input = {\"Scope\": orig_scope, \"Predicate1\": orig_pred1, \"Predicate2\": orig_pred2}\n",
    "    counterfactual_input = {\"Scope\": new_scope, \"Predicate1\": new_pred1, \"Predicate2\": new_pred2}\n",
    "\n",
    "    original_result = scone_model.run_forward(intervention=original_input)\n",
    "    counterfactual_result = scone_model.run_forward(intervention=counterfactual_input)\n",
    "\n",
    "    return {\"input\": original_result, \"counterfactual_inputs\": [counterfactual_result]}\n",
    "\n",
    "def build_counterfactual_datasets(datasets_dict, scone_model, size=100):\n",
    "    \"\"\"Build counterfactual datasets for SCONE with base entailment type.\"\"\"\n",
    "    counterfactual_dataset = {\n",
    "        \"base_entailment_counterfactuals\": CounterfactualDataset.from_sampler(\n",
    "            size, lambda: create_base_entailment_counterfactual(datasets_dict, scone_model)\n",
    "        ),\n",
    "        \"scope_counterfactuals\": CounterfactualDataset.from_sampler(\n",
    "            size, lambda: create_scope_counterfactual(datasets_dict, scone_model)\n",
    "        )\n",
    "    }\n",
    "    return counterfactual_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d19500",
   "metadata": {},
   "source": [
    "## 8. Causal Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4bb328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create valid predicate pairs from entailment data\n",
    "valid_pairs = []\n",
    "for entry in entailment_data:\n",
    "    if entry['label'] == 1:\n",
    "        valid_pairs.append((entry['hyponym'], entry['hypernym']))\n",
    "\n",
    "print(f\"Found {len(valid_pairs)} valid predicate pairs\")\n",
    "\n",
    "# Define causal model components\n",
    "variables = [\"Predicate1\", \"Predicate2\", \"Scope\", \"BaseEntailment\", \n",
    "            \"FinalRelation\", \"PremiseSentence\", \"HypothesisSentence\", \n",
    "            \"raw_input\", \"raw_output\"]\n",
    "\n",
    "scope_types = [\"no_negation\", \"one_scoped\", \"one_not_scoped\"]\n",
    "\n",
    "values = {\n",
    "    \"Predicate1\": [pair[0] for pair in valid_pairs],\n",
    "    \"Predicate2\": [pair[1] for pair in valid_pairs], \n",
    "    \"Scope\": scope_types,\n",
    "    \"BaseEntailment\": [\"entailment\", \"neutral\"],\n",
    "    \"FinalRelation\": [\"entailment\", \"neutral\"],\n",
    "    \"PremiseSentence\": None,\n",
    "    \"HypothesisSentence\": None,\n",
    "    \"raw_input\": None,\n",
    "    \"raw_output\": [\"entailment\", \"neutral\"]\n",
    "}\n",
    "\n",
    "parents = {\n",
    "    \"Predicate1\": [],\n",
    "    \"Predicate2\": [],\n",
    "    \"Scope\": [],\n",
    "    \"BaseEntailment\": [\"Predicate1\", \"Predicate2\"],\n",
    "    \"FinalRelation\": [\"BaseEntailment\", \"Scope\"],\n",
    "    \"PremiseSentence\": [\"Predicate1\", \"Predicate2\", \"Scope\"],\n",
    "    \"HypothesisSentence\": [\"Predicate1\", \"Predicate2\", \"Scope\"],\n",
    "    \"raw_input\": [\"PremiseSentence\", \"HypothesisSentence\"],\n",
    "    \"raw_output\": [\"FinalRelation\"]\n",
    "}\n",
    "\n",
    "mechanisms = {\n",
    "    \"Predicate1\": lambda: random.choice([pair[0] for pair in valid_pairs]),\n",
    "    \"Predicate2\": lambda: random.choice([pair[1] for pair in valid_pairs]),\n",
    "    \"Scope\": lambda: random.choice(scope_types),\n",
    "    \"BaseEntailment\": compute_base_entailment,\n",
    "    \"FinalRelation\": compute_final_relation,\n",
    "    \"PremiseSentence\": lambda p1, p2, scope: get_premise_sentence(p1, p2, scope, datasets_dict),\n",
    "    \"HypothesisSentence\": lambda p1, p2, scope: get_hypothesis_sentence(p1, p2, scope, datasets_dict),\n",
    "    \"raw_input\": create_raw_input,\n",
    "    \"raw_output\": create_raw_output\n",
    "}\n",
    "\n",
    "# Create SCONE causal model\n",
    "scone_model = SconeCausalModel(datasets_dict, entailment_data, variables, values, parents, mechanisms)\n",
    "\n",
    "print(\"Causal model created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca5666d",
   "metadata": {},
   "source": [
    "## 9. Dataset Filtering and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7c504f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_filter_experiment(pipeline, scone_model, datasets):\n",
    "    \"\"\"Filter datasets based on model performance using custom SCONE filter.\"\"\"\n",
    "    print(\"\\nFiltering datasets based on model performance...\")\n",
    "    exp = SconeFilterExperiment(pipeline, scone_model, checker, debug=True)\n",
    "    return exp.filter(datasets, verbose=True, batch_size=1)\n",
    "\n",
    "def save_filtered_datasets(filtered_datasets):\n",
    "    \"\"\"Save filtered datasets to data/counterfactuals_filtered folder.\"\"\"\n",
    "    save_dir = \"data/counterfactuals_filtered\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for dataset_name, dataset in filtered_datasets.items():\n",
    "        dataset_path = os.path.join(save_dir, f\"{dataset_name}.jsonl\")\n",
    "        with open(dataset_path, 'w') as f:\n",
    "            for example in dataset:\n",
    "                json.dump(example, f)\n",
    "                f.write('\\n')\n",
    "        print(f\"Saved {len(dataset)} examples to {dataset_path}\")\n",
    "\n",
    "# Check if filtered datasets already exist\n",
    "filtered_datasets_path = \"data/counterfactuals_filtered\"\n",
    "if not os.path.exists(filtered_datasets_path) or not os.listdir(filtered_datasets_path):\n",
    "    print(\"No filtered datasets found - generating new counterfactual datasets...\")\n",
    "    counterfactual_datasets = build_counterfactual_datasets(datasets_dict, scone_model, size=200)\n",
    "    filtered_datasets = run_filter_experiment(pipeline, scone_model, counterfactual_datasets)\n",
    "    save_filtered_datasets(filtered_datasets)\n",
    "else:\n",
    "    print(\"Loading existing filtered datasets...\")\n",
    "    filtered_datasets = {}\n",
    "    for filename in os.listdir(filtered_datasets_path):\n",
    "        dataset_name = filename.replace('.jsonl', '')\n",
    "        dataset_path = os.path.join(filtered_datasets_path, filename)\n",
    "        examples = []\n",
    "        with open(dataset_path, 'r') as f:\n",
    "            for line in f:\n",
    "                examples.append(json.loads(line))\n",
    "        filtered_datasets[dataset_name] = examples\n",
    "        print(f\"Loaded {len(examples)} examples from {filename}\")\n",
    "\n",
    "print(f\"\\nFiltered datasets ready: {list(filtered_datasets.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9419e550",
   "metadata": {},
   "source": [
    "## 10. Token Position Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4417baaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define token positions for intervention\n",
    "token_positions = [\n",
    "    TokenPosition(lambda x: get_predicate_premise(x, pipeline), pipeline, id=\"premise_predicate\"),\n",
    "    TokenPosition(lambda x: get_predicate_hypothesis(x, pipeline), pipeline, id=\"hypothesis_predicate\"),\n",
    "    TokenPosition(lambda x: get_negation_index(x, pipeline), pipeline, id=\"negation\"),\n",
    "    TokenPosition(lambda x: get_last_token_index(x, pipeline), pipeline, id=\"last_token\")\n",
    "]\n",
    "\n",
    "print(f\"Created {len(token_positions)} token position functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeef2457",
   "metadata": {},
   "source": [
    "## 11. Experiment Configuration and Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca243019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configuration\n",
    "start_layer = 22\n",
    "end_layer = pipeline.get_num_layers()  # Full layers experiment\n",
    "target_variables_list = [[\"FinalRelation\"], [\"BaseEntailment\"], [\"Scope\"]]\n",
    "results_dir = \"SCONE_demo_results\"\n",
    "\n",
    "config = {\n",
    "    \"batch_size\": 16, \n",
    "    \"evaluation_batch_size\": 128, \n",
    "    \"training_epoch\": 32, \n",
    "    \"n_features\": 16, \n",
    "}\n",
    "\n",
    "print(f\"Experiment configuration:\")\n",
    "print(f\"  Layers: {start_layer} to {end_layer}\")\n",
    "print(f\"  Target variables: {target_variables_list}\")\n",
    "print(f\"  Results directory: {results_dir}\")\n",
    "print(f\"  Config: {config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ef8139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run the experiment\n",
    "experiment = PatchResidualStream(\n",
    "    pipeline, \n",
    "    scone_model, \n",
    "    list(range(start_layer, end_layer)), \n",
    "    token_positions, \n",
    "    checker, \n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(\"Experiment object created. Starting intervention training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5917cb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train interventions\n",
    "print(\"Training interventions for Scope variable...\")\n",
    "experiment.train_interventions(\n",
    "    filtered_datasets, \n",
    "    [\"Scope\"], \n",
    "    method=\"DAS\", \n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89b0b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform interventions and get results\n",
    "# This will save the results in json format\n",
    "print(\"Performing interventions...\")\n",
    "raw_results = experiment.perform_interventions(\n",
    "    filtered_datasets, \n",
    "    verbose=True, \n",
    "    target_variables_list=target_variables_list, \n",
    "    save_dir=results_dir\n",
    ")\n",
    "\n",
    "print(f\"Interventions completed! Results saved to {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f70616d",
   "metadata": {},
   "source": [
    "## 12. Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d5a38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results summary\n",
    "print(\"\\n=== EXPERIMENT RESULTS SUMMARY ===\")\n",
    "print(f\"Results saved to: {results_dir}\")\n",
    "print(f\"Target variables analyzed: {target_variables_list}\")\n",
    "print(f\"Layers analyzed: {start_layer} to {end_layer-1}\")\n",
    "print(f\"Token positions: {[pos.id for pos in token_positions]}\")\n",
    "\n",
    "if raw_results:\n",
    "    print(f\"\\nRaw results keys: {list(raw_results.keys())}\")\n",
    "    first_target = target_variables_list[0]\n",
    "    target_key = \"_\".join(first_target)\n",
    "    \n",
    "    if target_key in raw_results:\n",
    "        sample_result = raw_results[target_key]\n",
    "        print(f\"\\nSample results for {first_target}:\")\n",
    "        print(f\"  Type: {type(sample_result)}\")\n",
    "        if hasattr(sample_result, 'shape'):\n",
    "            print(f\"  Shape: {sample_result.shape}\")\n",
    "        elif isinstance(sample_result, dict):\n",
    "            print(f\"  Keys: {list(sample_result.keys())}\")\n",
    "\n",
    "print(\"\\nExperiment completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4738c48c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements a causal abstraction experiment on the SCONE dataset to understand how language models process logical entailment and negation scope. The experiment:\n",
    "\n",
    "1. Loads SCONE data with different negation scoping patterns\n",
    "2. Defines a causal model with variables for predicates, scope, base entailment, and final relation\n",
    "3. Generates counterfactual datasets by systematically varying these causal variables\n",
    "4. Filters datasets to ensure the model can solve the base task\n",
    "5. Trains interventions using Distributed Alignment Search (DAS)\n",
    "6. Performs causal interventions on neural representations at different layers\n",
    "7. Analyzes results to understand which layers encode which semantic variables"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (causal_abstraction)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
